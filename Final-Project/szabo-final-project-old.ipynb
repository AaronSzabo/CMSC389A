{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aaron Szabo Final Project\n",
    "\n",
    "**Course**: CMSC 389A Practical Deep Learning  \n",
    "\n",
    "**Imports**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_white = 'winequality-white.csv'\n",
    "file_name_red = 'winequality-red.csv'\n",
    "columns = [\n",
    "    'Fixed acidity',\n",
    "    'Volatile acidity',\n",
    "    'Citric acid',\n",
    "    'Residual sugar',\n",
    "    'Chlorides',\n",
    "    'Free sulfur dioxide',\n",
    "    'Total sulfur dioxide',\n",
    "    'Density',\n",
    "    'pH',\n",
    "    'Sulphates',\n",
    "    'Alcohol',\n",
    "    'Quality'\n",
    "]\n",
    "\n",
    "df_white = pd.read_csv(file_name_white, names=columns, delimiter=';', header=0)\n",
    "df_red = pd.read_csv(file_name_red, names=columns, delimiter=';', header=0)\n",
    "df_white[columns[:-1]] = df_white[columns[:-1]].astype(float)\n",
    "df_red[columns[:-1]] = df_red[columns[:-1]].astype(float)\n",
    "df_white[columns[-1]] = df_white[columns[-1]].astype(str)\n",
    "df_red[columns[-1]] = df_red[columns[-1]].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all the values between -0.5 and 0.5\n",
    "for feature in df_white.columns[:-1]:\n",
    "    max_value = df_white[feature].max()\n",
    "    min_value = df_white[feature].min()\n",
    "    mean_value = df_white[feature].mean()\n",
    "    df_white[feature] = (df_white[feature] - mean_value) / (max_value - min_value)\n",
    "for feature in df_red.columns[:-1]:\n",
    "    max_value = df_red[feature].max()\n",
    "    min_value = df_red[feature].min()\n",
    "    mean_value = df_red[feature].mean()\n",
    "    df_red[feature] = (df_red[feature] - mean_value) / (max_value - min_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now examine the data after normalizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fixed acidity</th>\n",
       "      <th>Volatile acidity</th>\n",
       "      <th>Citric acid</th>\n",
       "      <th>Residual sugar</th>\n",
       "      <th>Chlorides</th>\n",
       "      <th>Free sulfur dioxide</th>\n",
       "      <th>Total sulfur dioxide</th>\n",
       "      <th>Density</th>\n",
       "      <th>pH</th>\n",
       "      <th>Sulphates</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.081384</td>\n",
       "      <td>0.117931</td>\n",
       "      <td>-0.270976</td>\n",
       "      <td>-0.043754</td>\n",
       "      <td>-0.019143</td>\n",
       "      <td>-0.068661</td>\n",
       "      <td>-0.044056</td>\n",
       "      <td>0.077336</td>\n",
       "      <td>0.156604</td>\n",
       "      <td>-0.058772</td>\n",
       "      <td>-0.157382</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.045986</td>\n",
       "      <td>0.241219</td>\n",
       "      <td>-0.270976</td>\n",
       "      <td>0.004191</td>\n",
       "      <td>0.017585</td>\n",
       "      <td>0.128522</td>\n",
       "      <td>0.072552</td>\n",
       "      <td>0.003915</td>\n",
       "      <td>-0.087491</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>-0.095844</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.045986</td>\n",
       "      <td>0.159027</td>\n",
       "      <td>-0.230976</td>\n",
       "      <td>-0.016357</td>\n",
       "      <td>0.007568</td>\n",
       "      <td>-0.012323</td>\n",
       "      <td>0.026616</td>\n",
       "      <td>0.018599</td>\n",
       "      <td>-0.040247</td>\n",
       "      <td>-0.004880</td>\n",
       "      <td>-0.095844</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.254899</td>\n",
       "      <td>-0.169740</td>\n",
       "      <td>0.289024</td>\n",
       "      <td>-0.043754</td>\n",
       "      <td>-0.020812</td>\n",
       "      <td>0.015846</td>\n",
       "      <td>0.047817</td>\n",
       "      <td>0.092021</td>\n",
       "      <td>-0.118987</td>\n",
       "      <td>-0.046796</td>\n",
       "      <td>-0.095844</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.081384</td>\n",
       "      <td>0.117931</td>\n",
       "      <td>-0.270976</td>\n",
       "      <td>-0.043754</td>\n",
       "      <td>-0.019143</td>\n",
       "      <td>-0.068661</td>\n",
       "      <td>-0.044056</td>\n",
       "      <td>0.077336</td>\n",
       "      <td>0.156604</td>\n",
       "      <td>-0.058772</td>\n",
       "      <td>-0.157382</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fixed acidity  Volatile acidity  Citric acid  Residual sugar  Chlorides  \\\n",
       "0      -0.081384          0.117931    -0.270976       -0.043754  -0.019143   \n",
       "1      -0.045986          0.241219    -0.270976        0.004191   0.017585   \n",
       "2      -0.045986          0.159027    -0.230976       -0.016357   0.007568   \n",
       "3       0.254899         -0.169740     0.289024       -0.043754  -0.020812   \n",
       "4      -0.081384          0.117931    -0.270976       -0.043754  -0.019143   \n",
       "\n",
       "   Free sulfur dioxide  Total sulfur dioxide   Density        pH  Sulphates  \\\n",
       "0            -0.068661             -0.044056  0.077336  0.156604  -0.058772   \n",
       "1             0.128522              0.072552  0.003915 -0.087491   0.013085   \n",
       "2            -0.012323              0.026616  0.018599 -0.040247  -0.004880   \n",
       "3             0.015846              0.047817  0.092021 -0.118987  -0.046796   \n",
       "4            -0.068661             -0.044056  0.077336  0.156604  -0.058772   \n",
       "\n",
       "    Alcohol Quality  \n",
       "0 -0.157382       5  \n",
       "1 -0.095844       5  \n",
       "2 -0.095844       5  \n",
       "3 -0.095844       6  \n",
       "4 -0.157382       5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_red.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the examples into a data variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example:\n",
    "    \"\"\"\n",
    "    Class to represent a data example.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, label):\n",
    "        \"\"\"\n",
    "        Create a new example.\n",
    "\n",
    "        :param label: The label (0 / 1) of the example\n",
    "        :param vocab: The real valued features of patient (list)\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_white = []\n",
    "for row in df_white.itertuples():\n",
    "    label = row[-1]\n",
    "    features = row[1:-1]\n",
    "    example = Example(features, label)\n",
    "    data_white.append(example)\n",
    "data_red = []\n",
    "for row in df_red.itertuples():\n",
    "    label = row[-1]\n",
    "    features = row[1:-1]\n",
    "    example = Example(features, label)\n",
    "    data_red.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data = shuffle(data, random_state=kSEED)\n",
    "\n",
    "X_W = [example.features for example in data_white]\n",
    "y_W = [example.label for example in data_white]\n",
    "X_R = [example.features for example in data_red]\n",
    "y_R = [example.label for example in data_red]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the Labels\n",
    "\n",
    "Before, our output variable only contained a binary label (either a 1 or a 0) that we wanted to predict. In this case, we have 3 potential output classes represented as strings. A common practice to use in this scenario is to create a one hot encoding for each class. A one-hot encoding is a matrix where each class is assigned an index and if the value at the index is True (>1), the data point belongs to that class. For example lets say our encoding looks like the following:\n",
    "\n",
    "Iris Setosa (index 0), Iris Versicolor (index 1), Iris Virginica (index 2)  \n",
    "If flower is Iris Setosa: `[1,0,0]`  \n",
    "If flower is Iris Versicolor: `[0,1,0]`  \n",
    "If flower is Iris Virginica: `[0,0,1]`  \n",
    "\n",
    "This is nice because our network can simply output a 1x3 vector (3 output nodes) where it contains the probability of being that class at each index. We can then just take the class with the highest probability and that is our class.\n",
    "\n",
    "Encoding the classes is really easy to do if you take advantage of sklearn's `Label Encoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  20  163 1457 2198  880  175    5]\n",
      "[ 10  53 681 638 199  18   0]\n"
     ]
    }
   ],
   "source": [
    "#3 is the lowest quality\n",
    "s1 = np.array([0,0,0,0,0,0,0])\n",
    "s2 = np.array([0,0,0,0,0,0,0])\n",
    "for val in y_W:\n",
    "    val_int = int(val)\n",
    "    s1[val_int-3] += 1\n",
    "for val in y_R:\n",
    "    val_int = int(val)\n",
    "    s2[val_int-3] += 1\n",
    "print(s1)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4898\n"
     ]
    }
   ],
   "source": [
    "print(len(y_W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder =  LabelEncoder()\n",
    "#encoded_y_W = encoder.fit_transform(y_W)\n",
    "#encoded_y_R = encoder.fit_transform(y_R)\n",
    "#dummy_y_W = pd.get_dummies(encoded_y_W).values\n",
    "#dummy_y_R = pd.get_dummies(encoded_y_R).values\n",
    "\n",
    "dummy_y_W = np.zeros((len(y_W),7), np.int32)\n",
    "dummy_y_R = np.zeros((len(y_R),7), np.int32)\n",
    "for i in range(len(y_W)):\n",
    "    val_int = int(y_W[i])\n",
    "    val_int -= 3\n",
    "    dummy_y_W[i,val_int] = 1\n",
    "for i in range(len(y_R)):\n",
    "    val_int = int(y_R[i])\n",
    "    val_int -= 3\n",
    "    dummy_y_R[i,val_int] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the encoded values for the fist 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 -> [0 0 0 1 0 0 0]\n",
      "6 -> [0 0 0 1 0 0 0]\n",
      "6 -> [0 0 0 1 0 0 0]\n",
      "6 -> [0 0 0 1 0 0 0]\n",
      "6 -> [0 0 0 1 0 0 0]\n",
      "5 -> [0 0 1 0 0 0 0]\n",
      "5 -> [0 0 1 0 0 0 0]\n",
      "5 -> [0 0 1 0 0 0 0]\n",
      "6 -> [0 0 0 1 0 0 0]\n",
      "5 -> [0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('{:} -> {:}'.format(y_W[i], dummy_y_W[i]))\n",
    "for i in range(5):\n",
    "    print('{:} -> {:}'.format(y_R[i], dummy_y_R[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "We will take a random 80% for training examples and a random 20% for testing. We can do this quickly using sklearn's train test split function.\n",
    " \n",
    "Note how we use `dummy_y` instead of `y` as it contains our encoded vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total White Examples: 4898\n",
      "Train Examples: 3918\n",
      "Test Examples:  980\n",
      "Total Red Examples: 1599\n",
      "Train Examples: 1279\n",
      "Test Examples:  320\n"
     ]
    }
   ],
   "source": [
    "X_train_W, X_test_W, y_train_W, y_test_W = train_test_split(X_W,dummy_y_W,test_size=0.2, random_state=10)\n",
    "X_train_R, X_test_R, y_train_R, y_test_R = train_test_split(X_R,dummy_y_R,test_size=0.2, random_state=10)\n",
    "\n",
    "X_train_W = np.array(X_train_W)\n",
    "y_train_W = np.array(y_train_W)\n",
    "X_test_W = np.array(X_test_W)\n",
    "y_test_W = np.array(y_test_W)\n",
    "\n",
    "print('Total White Examples: {:}\\nTrain Examples: {:}\\nTest Examples: {:4d}'.format(len(data_white), len(X_train_W), len(X_test_W)))\n",
    "print('Total Red Examples: {:}\\nTrain Examples: {:}\\nTest Examples: {:4d}'.format(len(data_red), len(X_train_R), len(X_test_R)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Model\n",
    "\n",
    "Notice how our model architecture looks extremely similar to the one we used for the binary classification task on the Diabetes dataset. The only big difference here is in the last line of the model where our final dense layer contains 3 neurons rather than 1. This is because each neuron will correspond to each of the Iris classes and contain a probability for each class.\n",
    "\n",
    "For example if the model output was `[0.8, 0.1, 0.1]`, then the predicted class is Iris Setosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(11,input_shape=(11,),activation='sigmoid'))\n",
    "model.add(Dense(30,activation='sigmoid'))\n",
    "#model.add(Dense(40,activation='sigmoid'))\n",
    "#model.add(Dense(50,activation='sigmoid'))\n",
    "#model.add(Dense(40,activation='sigmoid'))\n",
    "model.add(Dense(30,activation='relu'))\n",
    "model.add(Dense(11,activation='relu'))\n",
    "model.add(Dense(7,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model\n",
    "\n",
    "Before we used the `binary_crossentropy` loss in the Diabetes dataset. We will now use the `category_crossentropy` loss to handle more than two classes. Check out the link [here](http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy) for more information on the differences between the two but just remember that binary is for two classes and categorical is for more than two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def cus_metric(y_true, y_pred):\n",
    "#    diff = np.nonzero(y_true.eval())-np.nonzero(y_pred.eval())\n",
    "#    return diff^2\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary\n",
    "\n",
    "To get a quick overview of our model, we can use the `.summary()` property of Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_76 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 30)                360       \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 11)                341       \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 7)                 84        \n",
      "=================================================================\n",
      "Total params: 1,847\n",
      "Trainable params: 1,847\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "We will train then neural network on the data we setup before. We will train for 100 epochs with a batch size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "3918/3918 [==============================] - 1s 270us/step - loss: 1.3606 - acc: 0.4288\n",
      "Epoch 2/500\n",
      "3918/3918 [==============================] - 0s 110us/step - loss: 1.2975 - acc: 0.4461\n",
      "Epoch 3/500\n",
      "3918/3918 [==============================] - 0s 108us/step - loss: 1.2939 - acc: 0.4461\n",
      "Epoch 4/500\n",
      "3918/3918 [==============================] - 0s 109us/step - loss: 1.2884 - acc: 0.4464\n",
      "Epoch 5/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.2618 - acc: 0.4556\n",
      "Epoch 6/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.2187 - acc: 0.4788\n",
      "Epoch 7/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.1961 - acc: 0.4890\n",
      "Epoch 8/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.1837 - acc: 0.4992\n",
      "Epoch 9/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.1737 - acc: 0.5026\n",
      "Epoch 10/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.1638 - acc: 0.5079\n",
      "Epoch 11/500\n",
      "3918/3918 [==============================] - 0s 109us/step - loss: 1.1604 - acc: 0.5003\n",
      "Epoch 12/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.1522 - acc: 0.5143\n",
      "Epoch 13/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.1462 - acc: 0.5168\n",
      "Epoch 14/500\n",
      "3918/3918 [==============================] - 0s 108us/step - loss: 1.1420 - acc: 0.5227\n",
      "Epoch 15/500\n",
      "3918/3918 [==============================] - 0s 109us/step - loss: 1.1395 - acc: 0.5189\n",
      "Epoch 16/500\n",
      "3918/3918 [==============================] - 0s 106us/step - loss: 1.1352 - acc: 0.5186 0s - loss: 1.2126 - acc\n",
      "Epoch 17/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.1325 - acc: 0.5222\n",
      "Epoch 18/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.1326 - acc: 0.5232\n",
      "Epoch 19/500\n",
      "3918/3918 [==============================] - 0s 110us/step - loss: 1.1308 - acc: 0.5260\n",
      "Epoch 20/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.1277 - acc: 0.5301\n",
      "Epoch 21/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.1255 - acc: 0.5263\n",
      "Epoch 22/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.1257 - acc: 0.5276\n",
      "Epoch 23/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.1234 - acc: 0.5314\n",
      "Epoch 24/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.1236 - acc: 0.5245\n",
      "Epoch 25/500\n",
      "3918/3918 [==============================] - 0s 113us/step - loss: 1.1222 - acc: 0.5362\n",
      "Epoch 26/500\n",
      "3918/3918 [==============================] - 0s 111us/step - loss: 1.1212 - acc: 0.5255\n",
      "Epoch 27/500\n",
      "3918/3918 [==============================] - 0s 113us/step - loss: 1.1197 - acc: 0.5319\n",
      "Epoch 28/500\n",
      "3918/3918 [==============================] - 0s 126us/step - loss: 1.1180 - acc: 0.5306\n",
      "Epoch 29/500\n",
      "3918/3918 [==============================] - 1s 131us/step - loss: 1.1191 - acc: 0.5278\n",
      "Epoch 30/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.1183 - acc: 0.5286\n",
      "Epoch 31/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.1185 - acc: 0.5237\n",
      "Epoch 32/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.1167 - acc: 0.5324\n",
      "Epoch 33/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.1157 - acc: 0.5276\n",
      "Epoch 34/500\n",
      "3918/3918 [==============================] - 0s 108us/step - loss: 1.1142 - acc: 0.5263\n",
      "Epoch 35/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.1145 - acc: 0.5268\n",
      "Epoch 36/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.1103 - acc: 0.5301\n",
      "Epoch 37/500\n",
      "3918/3918 [==============================] - 0s 106us/step - loss: 1.1143 - acc: 0.5294\n",
      "Epoch 38/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.1140 - acc: 0.5263\n",
      "Epoch 39/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.1110 - acc: 0.5342\n",
      "Epoch 40/500\n",
      "3918/3918 [==============================] - 0s 111us/step - loss: 1.1123 - acc: 0.5291\n",
      "Epoch 41/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.1099 - acc: 0.5219\n",
      "Epoch 42/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.1091 - acc: 0.5332\n",
      "Epoch 43/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.1102 - acc: 0.5314\n",
      "Epoch 44/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.1094 - acc: 0.5368\n",
      "Epoch 45/500\n",
      "3918/3918 [==============================] - 0s 106us/step - loss: 1.1078 - acc: 0.5368\n",
      "Epoch 46/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.1086 - acc: 0.5296\n",
      "Epoch 47/500\n",
      "3918/3918 [==============================] - 0s 111us/step - loss: 1.1079 - acc: 0.5265\n",
      "Epoch 48/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.1100 - acc: 0.5286\n",
      "Epoch 49/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.1048 - acc: 0.5345\n",
      "Epoch 50/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.1073 - acc: 0.5334\n",
      "Epoch 51/500\n",
      "3918/3918 [==============================] - 0s 117us/step - loss: 1.1058 - acc: 0.5342\n",
      "Epoch 52/500\n",
      "3918/3918 [==============================] - 0s 126us/step - loss: 1.1050 - acc: 0.5360\n",
      "Epoch 53/500\n",
      "3918/3918 [==============================] - 0s 115us/step - loss: 1.1047 - acc: 0.5342\n",
      "Epoch 54/500\n",
      "3918/3918 [==============================] - 0s 117us/step - loss: 1.1043 - acc: 0.5306\n",
      "Epoch 55/500\n",
      "3918/3918 [==============================] - 0s 109us/step - loss: 1.1044 - acc: 0.5337\n",
      "Epoch 56/500\n",
      "3918/3918 [==============================] - 0s 125us/step - loss: 1.1025 - acc: 0.5334\n",
      "Epoch 57/500\n",
      "3918/3918 [==============================] - 0s 111us/step - loss: 1.1034 - acc: 0.5319\n",
      "Epoch 58/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.1047 - acc: 0.5327\n",
      "Epoch 59/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.1063 - acc: 0.5278\n",
      "Epoch 60/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.1031 - acc: 0.5342\n",
      "Epoch 61/500\n",
      "3918/3918 [==============================] - 0s 116us/step - loss: 1.1041 - acc: 0.5342\n",
      "Epoch 62/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.1037 - acc: 0.5342\n",
      "Epoch 63/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.1022 - acc: 0.5375\n",
      "Epoch 64/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.1018 - acc: 0.5347\n",
      "Epoch 65/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.1011 - acc: 0.5380\n",
      "Epoch 66/500\n",
      "3918/3918 [==============================] - 0s 113us/step - loss: 1.1019 - acc: 0.5319\n",
      "Epoch 67/500\n",
      "3918/3918 [==============================] - 0s 119us/step - loss: 1.1003 - acc: 0.5396\n",
      "Epoch 68/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.1014 - acc: 0.5350\n",
      "Epoch 69/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0987 - acc: 0.5373\n",
      "Epoch 70/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.1005 - acc: 0.5347\n",
      "Epoch 71/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.1001 - acc: 0.5350\n",
      "Epoch 72/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0997 - acc: 0.5299\n",
      "Epoch 73/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.1012 - acc: 0.5342\n",
      "Epoch 74/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0997 - acc: 0.5294\n",
      "Epoch 75/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.1001 - acc: 0.5413\n",
      "Epoch 76/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0995 - acc: 0.5327\n",
      "Epoch 77/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0990 - acc: 0.5316\n",
      "Epoch 78/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0973 - acc: 0.5393\n",
      "Epoch 79/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0984 - acc: 0.5403\n",
      "Epoch 80/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0968 - acc: 0.5362\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0987 - acc: 0.5391: 0s - loss: 1.0997 - acc: \n",
      "Epoch 82/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0974 - acc: 0.5416\n",
      "Epoch 83/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0971 - acc: 0.5327\n",
      "Epoch 84/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0972 - acc: 0.5380\n",
      "Epoch 85/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.0973 - acc: 0.5360\n",
      "Epoch 86/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0985 - acc: 0.5316\n",
      "Epoch 87/500\n",
      "3918/3918 [==============================] - 0s 96us/step - loss: 1.0950 - acc: 0.5347\n",
      "Epoch 88/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0970 - acc: 0.5373\n",
      "Epoch 89/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0944 - acc: 0.5373\n",
      "Epoch 90/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0973 - acc: 0.5352\n",
      "Epoch 91/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0957 - acc: 0.5391\n",
      "Epoch 92/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0944 - acc: 0.5421\n",
      "Epoch 93/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0935 - acc: 0.5401\n",
      "Epoch 94/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0951 - acc: 0.5449\n",
      "Epoch 95/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0950 - acc: 0.5398\n",
      "Epoch 96/500\n",
      "3918/3918 [==============================] - 0s 97us/step - loss: 1.0934 - acc: 0.5403\n",
      "Epoch 97/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0958 - acc: 0.5357\n",
      "Epoch 98/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0932 - acc: 0.5459\n",
      "Epoch 99/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0952 - acc: 0.5439\n",
      "Epoch 100/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0939 - acc: 0.5434\n",
      "Epoch 101/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0921 - acc: 0.5375\n",
      "Epoch 102/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0922 - acc: 0.5444\n",
      "Epoch 103/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0945 - acc: 0.5396\n",
      "Epoch 104/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0929 - acc: 0.5411\n",
      "Epoch 105/500\n",
      "3918/3918 [==============================] - 0s 96us/step - loss: 1.0921 - acc: 0.5421\n",
      "Epoch 106/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0936 - acc: 0.5419\n",
      "Epoch 107/500\n",
      "3918/3918 [==============================] - 0s 115us/step - loss: 1.0911 - acc: 0.5429\n",
      "Epoch 108/500\n",
      "3918/3918 [==============================] - 0s 113us/step - loss: 1.0896 - acc: 0.5393\n",
      "Epoch 109/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0908 - acc: 0.5421\n",
      "Epoch 110/500\n",
      "3918/3918 [==============================] - 0s 97us/step - loss: 1.0902 - acc: 0.5452\n",
      "Epoch 111/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0911 - acc: 0.5426\n",
      "Epoch 112/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0907 - acc: 0.5459\n",
      "Epoch 113/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0908 - acc: 0.5439\n",
      "Epoch 114/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0899 - acc: 0.5413\n",
      "Epoch 115/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0907 - acc: 0.5419\n",
      "Epoch 116/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0901 - acc: 0.5385\n",
      "Epoch 117/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0882 - acc: 0.5459\n",
      "Epoch 118/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0887 - acc: 0.5442\n",
      "Epoch 119/500\n",
      "3918/3918 [==============================] - 0s 113us/step - loss: 1.0884 - acc: 0.5485\n",
      "Epoch 120/500\n",
      "3918/3918 [==============================] - 0s 110us/step - loss: 1.0892 - acc: 0.5447\n",
      "Epoch 121/500\n",
      "3918/3918 [==============================] - 0s 114us/step - loss: 1.0877 - acc: 0.5421\n",
      "Epoch 122/500\n",
      "3918/3918 [==============================] - 1s 138us/step - loss: 1.0888 - acc: 0.5434\n",
      "Epoch 123/500\n",
      "3918/3918 [==============================] - 1s 133us/step - loss: 1.0870 - acc: 0.5447\n",
      "Epoch 124/500\n",
      "3918/3918 [==============================] - 0s 110us/step - loss: 1.0871 - acc: 0.5449\n",
      "Epoch 125/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0857 - acc: 0.5424\n",
      "Epoch 126/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0846 - acc: 0.5510\n",
      "Epoch 127/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0866 - acc: 0.5444\n",
      "Epoch 128/500\n",
      "3918/3918 [==============================] - 0s 111us/step - loss: 1.0865 - acc: 0.5442\n",
      "Epoch 129/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0872 - acc: 0.5449\n",
      "Epoch 130/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0839 - acc: 0.5487\n",
      "Epoch 131/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0841 - acc: 0.5439\n",
      "Epoch 132/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0847 - acc: 0.5465\n",
      "Epoch 133/500\n",
      "3918/3918 [==============================] - 0s 108us/step - loss: 1.0851 - acc: 0.5457\n",
      "Epoch 134/500\n",
      "3918/3918 [==============================] - 0s 113us/step - loss: 1.0850 - acc: 0.5480\n",
      "Epoch 135/500\n",
      "3918/3918 [==============================] - 1s 134us/step - loss: 1.0814 - acc: 0.5480\n",
      "Epoch 136/500\n",
      "3918/3918 [==============================] - 0s 113us/step - loss: 1.0849 - acc: 0.5429\n",
      "Epoch 137/500\n",
      "3918/3918 [==============================] - 0s 120us/step - loss: 1.0836 - acc: 0.5436\n",
      "Epoch 138/500\n",
      "3918/3918 [==============================] - 0s 111us/step - loss: 1.0833 - acc: 0.5439\n",
      "Epoch 139/500\n",
      "3918/3918 [==============================] - 0s 111us/step - loss: 1.0821 - acc: 0.5505\n",
      "Epoch 140/500\n",
      "3918/3918 [==============================] - 0s 111us/step - loss: 1.0813 - acc: 0.5516\n",
      "Epoch 141/500\n",
      "3918/3918 [==============================] - 0s 113us/step - loss: 1.0838 - acc: 0.5480\n",
      "Epoch 142/500\n",
      "3918/3918 [==============================] - 0s 123us/step - loss: 1.0823 - acc: 0.5462\n",
      "Epoch 143/500\n",
      "3918/3918 [==============================] - 0s 109us/step - loss: 1.0819 - acc: 0.5510\n",
      "Epoch 144/500\n",
      "3918/3918 [==============================] - 0s 122us/step - loss: 1.0810 - acc: 0.5493\n",
      "Epoch 145/500\n",
      "3918/3918 [==============================] - 0s 122us/step - loss: 1.0813 - acc: 0.5459\n",
      "Epoch 146/500\n",
      "3918/3918 [==============================] - 0s 111us/step - loss: 1.0813 - acc: 0.5472\n",
      "Epoch 147/500\n",
      "3918/3918 [==============================] - 0s 111us/step - loss: 1.0797 - acc: 0.5462\n",
      "Epoch 148/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0799 - acc: 0.5462\n",
      "Epoch 149/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0802 - acc: 0.5477\n",
      "Epoch 150/500\n",
      "3918/3918 [==============================] - 0s 106us/step - loss: 1.0804 - acc: 0.5449\n",
      "Epoch 151/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0786 - acc: 0.5454\n",
      "Epoch 152/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0780 - acc: 0.5516\n",
      "Epoch 153/500\n",
      "3918/3918 [==============================] - 0s 108us/step - loss: 1.0774 - acc: 0.5498\n",
      "Epoch 154/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0764 - acc: 0.5477\n",
      "Epoch 155/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0768 - acc: 0.5513\n",
      "Epoch 156/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0784 - acc: 0.5495\n",
      "Epoch 157/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0774 - acc: 0.5477\n",
      "Epoch 158/500\n",
      "3918/3918 [==============================] - 0s 109us/step - loss: 1.0771 - acc: 0.5531\n",
      "Epoch 159/500\n",
      "3918/3918 [==============================] - 0s 97us/step - loss: 1.0753 - acc: 0.5472\n",
      "Epoch 160/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0763 - acc: 0.5459\n",
      "Epoch 161/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0753 - acc: 0.5485\n",
      "Epoch 162/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0771 - acc: 0.5508\n",
      "Epoch 163/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0759 - acc: 0.5457\n",
      "Epoch 164/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0743 - acc: 0.5475\n",
      "Epoch 165/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0747 - acc: 0.5490\n",
      "Epoch 166/500\n",
      "3918/3918 [==============================] - 0s 117us/step - loss: 1.0756 - acc: 0.5516\n",
      "Epoch 167/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0743 - acc: 0.5510\n",
      "Epoch 168/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0725 - acc: 0.5462\n",
      "Epoch 169/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0719 - acc: 0.5518\n",
      "Epoch 170/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0730 - acc: 0.5498\n",
      "Epoch 171/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.0728 - acc: 0.5500\n",
      "Epoch 172/500\n",
      "3918/3918 [==============================] - 0s 106us/step - loss: 1.0724 - acc: 0.5508\n",
      "Epoch 173/500\n",
      "3918/3918 [==============================] - 0s 106us/step - loss: 1.0714 - acc: 0.5459\n",
      "Epoch 174/500\n",
      "3918/3918 [==============================] - 0s 125us/step - loss: 1.0697 - acc: 0.5493\n",
      "Epoch 175/500\n",
      "3918/3918 [==============================] - 0s 118us/step - loss: 1.0710 - acc: 0.5541\n",
      "Epoch 176/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0728 - acc: 0.5523\n",
      "Epoch 177/500\n",
      "3918/3918 [==============================] - 0s 117us/step - loss: 1.0713 - acc: 0.5493\n",
      "Epoch 178/500\n",
      "3918/3918 [==============================] - 0s 113us/step - loss: 1.0693 - acc: 0.5531\n",
      "Epoch 179/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.0718 - acc: 0.5482\n",
      "Epoch 180/500\n",
      "3918/3918 [==============================] - 0s 118us/step - loss: 1.0706 - acc: 0.5500\n",
      "Epoch 181/500\n",
      "3918/3918 [==============================] - 0s 120us/step - loss: 1.0726 - acc: 0.5503\n",
      "Epoch 182/500\n",
      "3918/3918 [==============================] - 0s 115us/step - loss: 1.0686 - acc: 0.5434\n",
      "Epoch 183/500\n",
      "3918/3918 [==============================] - 0s 119us/step - loss: 1.0697 - acc: 0.5503\n",
      "Epoch 184/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0686 - acc: 0.5480\n",
      "Epoch 185/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0694 - acc: 0.5510\n",
      "Epoch 186/500\n",
      "3918/3918 [==============================] - 0s 98us/step - loss: 1.0694 - acc: 0.5498\n",
      "Epoch 187/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0694 - acc: 0.5472\n",
      "Epoch 188/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0689 - acc: 0.5513\n",
      "Epoch 189/500\n",
      "3918/3918 [==============================] - 0s 118us/step - loss: 1.0685 - acc: 0.5482\n",
      "Epoch 190/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0693 - acc: 0.5551\n",
      "Epoch 191/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0672 - acc: 0.5533\n",
      "Epoch 192/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0663 - acc: 0.5498\n",
      "Epoch 193/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0672 - acc: 0.5528\n",
      "Epoch 194/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0676 - acc: 0.5518\n",
      "Epoch 195/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0687 - acc: 0.5526\n",
      "Epoch 196/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0683 - acc: 0.5513\n",
      "Epoch 197/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0656 - acc: 0.5505\n",
      "Epoch 198/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0681 - acc: 0.5485\n",
      "Epoch 199/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0644 - acc: 0.5521\n",
      "Epoch 200/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0672 - acc: 0.5516\n",
      "Epoch 201/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0661 - acc: 0.5516\n",
      "Epoch 202/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0662 - acc: 0.5498\n",
      "Epoch 203/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0650 - acc: 0.5500\n",
      "Epoch 204/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0625 - acc: 0.5523\n",
      "Epoch 205/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0636 - acc: 0.5523\n",
      "Epoch 206/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0642 - acc: 0.5518\n",
      "Epoch 207/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0649 - acc: 0.5487\n",
      "Epoch 208/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0619 - acc: 0.5544\n",
      "Epoch 209/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0628 - acc: 0.5472\n",
      "Epoch 210/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0620 - acc: 0.5559\n",
      "Epoch 211/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0630 - acc: 0.5505\n",
      "Epoch 212/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0637 - acc: 0.5526\n",
      "Epoch 213/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0633 - acc: 0.5523\n",
      "Epoch 214/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0653 - acc: 0.5516\n",
      "Epoch 215/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0620 - acc: 0.5526\n",
      "Epoch 216/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0614 - acc: 0.5510\n",
      "Epoch 217/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.0616 - acc: 0.5516\n",
      "Epoch 218/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0623 - acc: 0.5485\n",
      "Epoch 219/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0614 - acc: 0.5539\n",
      "Epoch 220/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0630 - acc: 0.5521\n",
      "Epoch 221/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0638 - acc: 0.5467\n",
      "Epoch 222/500\n",
      "3918/3918 [==============================] - 0s 120us/step - loss: 1.0614 - acc: 0.5551\n",
      "Epoch 223/500\n",
      "3918/3918 [==============================] - 0s 114us/step - loss: 1.0608 - acc: 0.5554\n",
      "Epoch 224/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0613 - acc: 0.5505\n",
      "Epoch 225/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0593 - acc: 0.5539\n",
      "Epoch 226/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0601 - acc: 0.5546\n",
      "Epoch 227/500\n",
      "3918/3918 [==============================] - 0s 109us/step - loss: 1.0589 - acc: 0.5503\n",
      "Epoch 228/500\n",
      "3918/3918 [==============================] - 0s 111us/step - loss: 1.0589 - acc: 0.5556\n",
      "Epoch 229/500\n",
      "3918/3918 [==============================] - 0s 113us/step - loss: 1.0587 - acc: 0.5539\n",
      "Epoch 230/500\n",
      "3918/3918 [==============================] - 0s 110us/step - loss: 1.0586 - acc: 0.5549\n",
      "Epoch 231/500\n",
      "3918/3918 [==============================] - 0s 110us/step - loss: 1.0611 - acc: 0.5536 0s - loss: 1.0498 - acc\n",
      "Epoch 232/500\n",
      "3918/3918 [==============================] - 0s 113us/step - loss: 1.0588 - acc: 0.5516\n",
      "Epoch 233/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0592 - acc: 0.5554\n",
      "Epoch 234/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0565 - acc: 0.5536\n",
      "Epoch 235/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0587 - acc: 0.5572\n",
      "Epoch 236/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0575 - acc: 0.5544\n",
      "Epoch 237/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0569 - acc: 0.5562\n",
      "Epoch 238/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0551 - acc: 0.5541\n",
      "Epoch 239/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0570 - acc: 0.5536\n",
      "Epoch 240/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0546 - acc: 0.5577\n",
      "Epoch 241/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0555 - acc: 0.5579\n",
      "Epoch 242/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0562 - acc: 0.5577\n",
      "Epoch 243/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0567 - acc: 0.5536\n",
      "Epoch 244/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0547 - acc: 0.5503\n",
      "Epoch 245/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0546 - acc: 0.5592\n",
      "Epoch 246/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0549 - acc: 0.5516\n",
      "Epoch 247/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0548 - acc: 0.5546\n",
      "Epoch 248/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0536 - acc: 0.5579\n",
      "Epoch 249/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0541 - acc: 0.5559\n",
      "Epoch 250/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0549 - acc: 0.5546\n",
      "Epoch 251/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0538 - acc: 0.5551\n",
      "Epoch 252/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0551 - acc: 0.5528\n",
      "Epoch 253/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0493 - acc: 0.5556\n",
      "Epoch 254/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0520 - acc: 0.5526\n",
      "Epoch 255/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0516 - acc: 0.5569\n",
      "Epoch 256/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0514 - acc: 0.5544\n",
      "Epoch 257/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0543 - acc: 0.5597\n",
      "Epoch 258/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0518 - acc: 0.5600\n",
      "Epoch 259/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0484 - acc: 0.5564\n",
      "Epoch 260/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0497 - acc: 0.5584\n",
      "Epoch 261/500\n",
      "3918/3918 [==============================] - 0s 118us/step - loss: 1.0511 - acc: 0.5541\n",
      "Epoch 262/500\n",
      "3918/3918 [==============================] - 0s 117us/step - loss: 1.0502 - acc: 0.5615\n",
      "Epoch 263/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0513 - acc: 0.5564\n",
      "Epoch 264/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0509 - acc: 0.5554\n",
      "Epoch 265/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0494 - acc: 0.5579\n",
      "Epoch 266/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0480 - acc: 0.5567\n",
      "Epoch 267/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0450 - acc: 0.5600\n",
      "Epoch 268/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0464 - acc: 0.5646\n",
      "Epoch 269/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0464 - acc: 0.5567\n",
      "Epoch 270/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0479 - acc: 0.5607\n",
      "Epoch 271/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0446 - acc: 0.5605\n",
      "Epoch 272/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0446 - acc: 0.5577\n",
      "Epoch 273/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0470 - acc: 0.5567\n",
      "Epoch 274/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0452 - acc: 0.5551\n",
      "Epoch 275/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0438 - acc: 0.5620\n",
      "Epoch 276/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0426 - acc: 0.5648\n",
      "Epoch 277/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0457 - acc: 0.5528\n",
      "Epoch 278/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0428 - acc: 0.5590\n",
      "Epoch 279/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0436 - acc: 0.5625\n",
      "Epoch 280/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0437 - acc: 0.5602\n",
      "Epoch 281/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0415 - acc: 0.5569\n",
      "Epoch 282/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0414 - acc: 0.5625\n",
      "Epoch 283/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0432 - acc: 0.5666\n",
      "Epoch 284/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0408 - acc: 0.5602\n",
      "Epoch 285/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0405 - acc: 0.5582\n",
      "Epoch 286/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.0422 - acc: 0.5574\n",
      "Epoch 287/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0419 - acc: 0.5595\n",
      "Epoch 288/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0399 - acc: 0.5607\n",
      "Epoch 289/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0417 - acc: 0.5661\n",
      "Epoch 290/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0400 - acc: 0.5641\n",
      "Epoch 291/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.0375 - acc: 0.5636\n",
      "Epoch 292/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0397 - acc: 0.5605\n",
      "Epoch 293/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0401 - acc: 0.5643\n",
      "Epoch 294/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0392 - acc: 0.5625\n",
      "Epoch 295/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0373 - acc: 0.5625\n",
      "Epoch 296/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0363 - acc: 0.5684\n",
      "Epoch 297/500\n",
      "3918/3918 [==============================] - 0s 106us/step - loss: 1.0365 - acc: 0.5628\n",
      "Epoch 298/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.0374 - acc: 0.5572\n",
      "Epoch 299/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0357 - acc: 0.5630\n",
      "Epoch 300/500\n",
      "3918/3918 [==============================] - 0s 109us/step - loss: 1.0376 - acc: 0.5610\n",
      "Epoch 301/500\n",
      "3918/3918 [==============================] - 0s 120us/step - loss: 1.0371 - acc: 0.5636\n",
      "Epoch 302/500\n",
      "3918/3918 [==============================] - 0s 117us/step - loss: 1.0345 - acc: 0.5692\n",
      "Epoch 303/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0345 - acc: 0.5661\n",
      "Epoch 304/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0356 - acc: 0.5666\n",
      "Epoch 305/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0325 - acc: 0.5643\n",
      "Epoch 306/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0337 - acc: 0.5702\n",
      "Epoch 307/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0342 - acc: 0.5633\n",
      "Epoch 308/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0350 - acc: 0.5656\n",
      "Epoch 309/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0344 - acc: 0.5620\n",
      "Epoch 310/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0320 - acc: 0.5702\n",
      "Epoch 311/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0345 - acc: 0.5630\n",
      "Epoch 312/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0340 - acc: 0.5653\n",
      "Epoch 313/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0328 - acc: 0.5661\n",
      "Epoch 314/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0297 - acc: 0.5638\n",
      "Epoch 315/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0312 - acc: 0.5656\n",
      "Epoch 316/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0334 - acc: 0.5661\n",
      "Epoch 317/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0324 - acc: 0.5681\n",
      "Epoch 318/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0306 - acc: 0.5651\n",
      "Epoch 319/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0318 - acc: 0.5658\n",
      "Epoch 320/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0286 - acc: 0.5784\n",
      "Epoch 321/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0299 - acc: 0.5610\n",
      "Epoch 322/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0285 - acc: 0.5722\n",
      "Epoch 323/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0288 - acc: 0.5712\n",
      "Epoch 324/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0297 - acc: 0.5628\n",
      "Epoch 325/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0282 - acc: 0.5684\n",
      "Epoch 326/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0290 - acc: 0.5658\n",
      "Epoch 327/500\n",
      "3918/3918 [==============================] - 0s 114us/step - loss: 1.0299 - acc: 0.5730\n",
      "Epoch 328/500\n",
      "3918/3918 [==============================] - 0s 115us/step - loss: 1.0299 - acc: 0.5674\n",
      "Epoch 329/500\n",
      "3918/3918 [==============================] - 0s 110us/step - loss: 1.0278 - acc: 0.5699\n",
      "Epoch 330/500\n",
      "3918/3918 [==============================] - 0s 111us/step - loss: 1.0268 - acc: 0.5664\n",
      "Epoch 331/500\n",
      "3918/3918 [==============================] - 0s 113us/step - loss: 1.0258 - acc: 0.5664\n",
      "Epoch 332/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0283 - acc: 0.5681\n",
      "Epoch 333/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0243 - acc: 0.5641\n",
      "Epoch 334/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0269 - acc: 0.5681\n",
      "Epoch 335/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0258 - acc: 0.5740\n",
      "Epoch 336/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0249 - acc: 0.5679\n",
      "Epoch 337/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0269 - acc: 0.5664\n",
      "Epoch 338/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0250 - acc: 0.5687\n",
      "Epoch 339/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0257 - acc: 0.5679\n",
      "Epoch 340/500\n",
      "3918/3918 [==============================] - 1s 132us/step - loss: 1.0249 - acc: 0.5694\n",
      "Epoch 341/500\n",
      "3918/3918 [==============================] - 0s 117us/step - loss: 1.0246 - acc: 0.5646\n",
      "Epoch 342/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0256 - acc: 0.5679\n",
      "Epoch 343/500\n",
      "3918/3918 [==============================] - 0s 122us/step - loss: 1.0219 - acc: 0.5710\n",
      "Epoch 344/500\n",
      "3918/3918 [==============================] - 1s 128us/step - loss: 1.0267 - acc: 0.5674\n",
      "Epoch 345/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0240 - acc: 0.5669\n",
      "Epoch 346/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0253 - acc: 0.5715\n",
      "Epoch 347/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0243 - acc: 0.5740\n",
      "Epoch 348/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.0245 - acc: 0.5704\n",
      "Epoch 349/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0226 - acc: 0.5707\n",
      "Epoch 350/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0224 - acc: 0.5669\n",
      "Epoch 351/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0211 - acc: 0.5743\n",
      "Epoch 352/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.0216 - acc: 0.5722\n",
      "Epoch 353/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0217 - acc: 0.5702\n",
      "Epoch 354/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0232 - acc: 0.5707\n",
      "Epoch 355/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0218 - acc: 0.5715\n",
      "Epoch 356/500\n",
      "3918/3918 [==============================] - 0s 108us/step - loss: 1.0210 - acc: 0.5681\n",
      "Epoch 357/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0226 - acc: 0.5679\n",
      "Epoch 358/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0211 - acc: 0.5697\n",
      "Epoch 359/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0209 - acc: 0.5720\n",
      "Epoch 360/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0208 - acc: 0.5720\n",
      "Epoch 361/500\n",
      "3918/3918 [==============================] - 0s 116us/step - loss: 1.0211 - acc: 0.5727\n",
      "Epoch 362/500\n",
      "3918/3918 [==============================] - 0s 106us/step - loss: 1.0224 - acc: 0.5735\n",
      "Epoch 363/500\n",
      "3918/3918 [==============================] - 0s 112us/step - loss: 1.0198 - acc: 0.5715\n",
      "Epoch 364/500\n",
      "3918/3918 [==============================] - 0s 112us/step - loss: 1.0199 - acc: 0.5687\n",
      "Epoch 365/500\n",
      "3918/3918 [==============================] - 0s 113us/step - loss: 1.0200 - acc: 0.5679\n",
      "Epoch 366/500\n",
      "3918/3918 [==============================] - 1s 128us/step - loss: 1.0178 - acc: 0.5733\n",
      "Epoch 367/500\n",
      "3918/3918 [==============================] - 1s 136us/step - loss: 1.0183 - acc: 0.5712\n",
      "Epoch 368/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0194 - acc: 0.5710\n",
      "Epoch 369/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.0176 - acc: 0.5722\n",
      "Epoch 370/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0187 - acc: 0.5671\n",
      "Epoch 371/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0180 - acc: 0.5720\n",
      "Epoch 372/500\n",
      "3918/3918 [==============================] - 0s 109us/step - loss: 1.0175 - acc: 0.5671\n",
      "Epoch 373/500\n",
      "3918/3918 [==============================] - 0s 108us/step - loss: 1.0177 - acc: 0.5605\n",
      "Epoch 374/500\n",
      "3918/3918 [==============================] - 0s 111us/step - loss: 1.0157 - acc: 0.5743\n",
      "Epoch 375/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0171 - acc: 0.5733\n",
      "Epoch 376/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.0161 - acc: 0.5717\n",
      "Epoch 377/500\n",
      "3918/3918 [==============================] - 0s 112us/step - loss: 1.0161 - acc: 0.5687\n",
      "Epoch 378/500\n",
      "3918/3918 [==============================] - 0s 116us/step - loss: 1.0173 - acc: 0.5646\n",
      "Epoch 379/500\n",
      "3918/3918 [==============================] - 0s 108us/step - loss: 1.0133 - acc: 0.5697\n",
      "Epoch 380/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0175 - acc: 0.5676\n",
      "Epoch 381/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0128 - acc: 0.5702\n",
      "Epoch 382/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0165 - acc: 0.5773\n",
      "Epoch 383/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0171 - acc: 0.5707\n",
      "Epoch 384/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0131 - acc: 0.5727\n",
      "Epoch 385/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0154 - acc: 0.5694\n",
      "Epoch 386/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0141 - acc: 0.5755\n",
      "Epoch 387/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0138 - acc: 0.5735\n",
      "Epoch 388/500\n",
      "3918/3918 [==============================] - 0s 109us/step - loss: 1.0141 - acc: 0.5694\n",
      "Epoch 389/500\n",
      "3918/3918 [==============================] - 0s 112us/step - loss: 1.0124 - acc: 0.5717\n",
      "Epoch 390/500\n",
      "3918/3918 [==============================] - 0s 106us/step - loss: 1.0114 - acc: 0.5761\n",
      "Epoch 391/500\n",
      "3918/3918 [==============================] - 0s 113us/step - loss: 1.0157 - acc: 0.5694\n",
      "Epoch 392/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0180 - acc: 0.5702\n",
      "Epoch 393/500\n",
      "3918/3918 [==============================] - 0s 112us/step - loss: 1.0144 - acc: 0.5761\n",
      "Epoch 394/500\n",
      "3918/3918 [==============================] - 0s 108us/step - loss: 1.0138 - acc: 0.5715\n",
      "Epoch 395/500\n",
      "3918/3918 [==============================] - 0s 112us/step - loss: 1.0105 - acc: 0.5748\n",
      "Epoch 396/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0116 - acc: 0.5745\n",
      "Epoch 397/500\n",
      "3918/3918 [==============================] - 0s 112us/step - loss: 1.0112 - acc: 0.5730\n",
      "Epoch 398/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0128 - acc: 0.5755\n",
      "Epoch 399/500\n",
      "3918/3918 [==============================] - 0s 114us/step - loss: 1.0115 - acc: 0.5753\n",
      "Epoch 400/500\n",
      "3918/3918 [==============================] - 0s 106us/step - loss: 1.0112 - acc: 0.5712\n",
      "Epoch 401/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3918/3918 [==============================] - 0s 110us/step - loss: 1.0114 - acc: 0.5763\n",
      "Epoch 402/500\n",
      "3918/3918 [==============================] - 0s 108us/step - loss: 1.0109 - acc: 0.5717\n",
      "Epoch 403/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0109 - acc: 0.5750\n",
      "Epoch 404/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0118 - acc: 0.5710\n",
      "Epoch 405/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0095 - acc: 0.5771\n",
      "Epoch 406/500\n",
      "3918/3918 [==============================] - 0s 106us/step - loss: 1.0093 - acc: 0.5738\n",
      "Epoch 407/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0125 - acc: 0.5753\n",
      "Epoch 408/500\n",
      "3918/3918 [==============================] - 0s 108us/step - loss: 1.0092 - acc: 0.5694\n",
      "Epoch 409/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0075 - acc: 0.5727\n",
      "Epoch 410/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0108 - acc: 0.5791\n",
      "Epoch 411/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0097 - acc: 0.5717\n",
      "Epoch 412/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0108 - acc: 0.5771\n",
      "Epoch 413/500\n",
      "3918/3918 [==============================] - 0s 106us/step - loss: 1.0096 - acc: 0.5684\n",
      "Epoch 414/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.0080 - acc: 0.5761\n",
      "Epoch 415/500\n",
      "3918/3918 [==============================] - 0s 107us/step - loss: 1.0085 - acc: 0.5766\n",
      "Epoch 416/500\n",
      "3918/3918 [==============================] - 0s 121us/step - loss: 1.0077 - acc: 0.5738\n",
      "Epoch 417/500\n",
      "3918/3918 [==============================] - 0s 109us/step - loss: 1.0091 - acc: 0.5727\n",
      "Epoch 418/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0088 - acc: 0.5730\n",
      "Epoch 419/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0075 - acc: 0.5755\n",
      "Epoch 420/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.0055 - acc: 0.5750\n",
      "Epoch 421/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0095 - acc: 0.5745\n",
      "Epoch 422/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0075 - acc: 0.5722\n",
      "Epoch 423/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0083 - acc: 0.5814\n",
      "Epoch 424/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0083 - acc: 0.5745\n",
      "Epoch 425/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0084 - acc: 0.5755\n",
      "Epoch 426/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0065 - acc: 0.5781\n",
      "Epoch 427/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0057 - acc: 0.5755\n",
      "Epoch 428/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0067 - acc: 0.5763\n",
      "Epoch 429/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.0069 - acc: 0.5776\n",
      "Epoch 430/500\n",
      "3918/3918 [==============================] - 0s 106us/step - loss: 1.0072 - acc: 0.5748\n",
      "Epoch 431/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0083 - acc: 0.5786\n",
      "Epoch 432/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0061 - acc: 0.5837\n",
      "Epoch 433/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0065 - acc: 0.5707\n",
      "Epoch 434/500\n",
      "3918/3918 [==============================] - 0s 105us/step - loss: 1.0069 - acc: 0.5771\n",
      "Epoch 435/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.0067 - acc: 0.5704\n",
      "Epoch 436/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0055 - acc: 0.5776\n",
      "Epoch 437/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0043 - acc: 0.5784\n",
      "Epoch 438/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0060 - acc: 0.5755\n",
      "Epoch 439/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0065 - acc: 0.5750\n",
      "Epoch 440/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0056 - acc: 0.5740\n",
      "Epoch 441/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0054 - acc: 0.5773\n",
      "Epoch 442/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0072 - acc: 0.5745\n",
      "Epoch 443/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0047 - acc: 0.5755\n",
      "Epoch 444/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0038 - acc: 0.5766\n",
      "Epoch 445/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0071 - acc: 0.5763\n",
      "Epoch 446/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0038 - acc: 0.5799\n",
      "Epoch 447/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0024 - acc: 0.5763\n",
      "Epoch 448/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0049 - acc: 0.5758\n",
      "Epoch 449/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0047 - acc: 0.5801\n",
      "Epoch 450/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0028 - acc: 0.5814\n",
      "Epoch 451/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0034 - acc: 0.5766\n",
      "Epoch 452/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0027 - acc: 0.5807\n",
      "Epoch 453/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0029 - acc: 0.5740\n",
      "Epoch 454/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0033 - acc: 0.5763\n",
      "Epoch 455/500\n",
      "3918/3918 [==============================] - 0s 118us/step - loss: 1.0038 - acc: 0.5814\n",
      "Epoch 456/500\n",
      "3918/3918 [==============================] - 0s 124us/step - loss: 1.0024 - acc: 0.5766\n",
      "Epoch 457/500\n",
      "3918/3918 [==============================] - 0s 118us/step - loss: 1.0017 - acc: 0.5791\n",
      "Epoch 458/500\n",
      "3918/3918 [==============================] - 0s 116us/step - loss: 1.0053 - acc: 0.5771\n",
      "Epoch 459/500\n",
      "3918/3918 [==============================] - 0s 111us/step - loss: 1.0041 - acc: 0.5781\n",
      "Epoch 460/500\n",
      "3918/3918 [==============================] - 0s 116us/step - loss: 1.0037 - acc: 0.5738\n",
      "Epoch 461/500\n",
      "3918/3918 [==============================] - 0s 110us/step - loss: 1.0035 - acc: 0.5776\n",
      "Epoch 462/500\n",
      "3918/3918 [==============================] - 0s 113us/step - loss: 1.0041 - acc: 0.5768\n",
      "Epoch 463/500\n",
      "3918/3918 [==============================] - 0s 108us/step - loss: 1.0030 - acc: 0.5771\n",
      "Epoch 464/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0034 - acc: 0.5789\n",
      "Epoch 465/500\n",
      "3918/3918 [==============================] - 0s 106us/step - loss: 0.9996 - acc: 0.5755\n",
      "Epoch 466/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0023 - acc: 0.5824\n",
      "Epoch 467/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0018 - acc: 0.5750\n",
      "Epoch 468/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0022 - acc: 0.5707\n",
      "Epoch 469/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0041 - acc: 0.5761\n",
      "Epoch 470/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0031 - acc: 0.5804\n",
      "Epoch 471/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0009 - acc: 0.5776\n",
      "Epoch 472/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.0024 - acc: 0.5789\n",
      "Epoch 473/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0016 - acc: 0.5789\n",
      "Epoch 474/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0016 - acc: 0.5832\n",
      "Epoch 475/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0026 - acc: 0.5766\n",
      "Epoch 476/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 0.9997 - acc: 0.5789\n",
      "Epoch 477/500\n",
      "3918/3918 [==============================] - 0s 104us/step - loss: 1.0021 - acc: 0.5748\n",
      "Epoch 478/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0008 - acc: 0.5801\n",
      "Epoch 479/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0019 - acc: 0.5791\n",
      "Epoch 480/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0011 - acc: 0.5781\n",
      "Epoch 481/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3918/3918 [==============================] - 0s 100us/step - loss: 0.9996 - acc: 0.5809\n",
      "Epoch 482/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 0.9993 - acc: 0.5758\n",
      "Epoch 483/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0017 - acc: 0.5799\n",
      "Epoch 484/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0003 - acc: 0.5740\n",
      "Epoch 485/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0013 - acc: 0.5807\n",
      "Epoch 486/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0020 - acc: 0.5791\n",
      "Epoch 487/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 1.0002 - acc: 0.5771\n",
      "Epoch 488/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 1.0002 - acc: 0.5801\n",
      "Epoch 489/500\n",
      "3918/3918 [==============================] - 0s 99us/step - loss: 1.0000 - acc: 0.5781\n",
      "Epoch 490/500\n",
      "3918/3918 [==============================] - 0s 100us/step - loss: 1.0010 - acc: 0.5801\n",
      "Epoch 491/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 1.0010 - acc: 0.5794\n",
      "Epoch 492/500\n",
      "3918/3918 [==============================] - 0s 103us/step - loss: 0.9996 - acc: 0.5809\n",
      "Epoch 493/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 0.9999 - acc: 0.5799\n",
      "Epoch 494/500\n",
      "3918/3918 [==============================] - 0s 115us/step - loss: 1.0029 - acc: 0.5750\n",
      "Epoch 495/500\n",
      "3918/3918 [==============================] - 0s 122us/step - loss: 0.9995 - acc: 0.5835\n",
      "Epoch 496/500\n",
      "3918/3918 [==============================] - 0s 122us/step - loss: 1.0011 - acc: 0.5745 0s - loss: 1.0110 - acc\n",
      "Epoch 497/500\n",
      "3918/3918 [==============================] - 0s 102us/step - loss: 0.9999 - acc: 0.5791\n",
      "Epoch 498/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 0.9996 - acc: 0.5761\n",
      "Epoch 499/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 0.9987 - acc: 0.5804\n",
      "Epoch 500/500\n",
      "3918/3918 [==============================] - 0s 101us/step - loss: 0.9973 - acc: 0.5796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1773ae3f5f8>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_W, y_train_W, epochs=500, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "We can now run the model on our test data and check the accuracy of the model. Since our `y_test` will be a matrix with a 1 at an element and our predicted output will also be a matrix but with probabilities at each index, we can check if we got the right class by selecting the index with the highest value. In the case of `y_test`, it will always be the correct class and in the case of our prediction, it will be the class we are most confident it is as the probabilities in the output vector sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5408163265306123\n"
     ]
    }
   ],
   "source": [
    "y_pred_W = model.predict(X_test_W)\n",
    "y_test_class_W = np.argmax(y_test_W,axis=1)\n",
    "y_pred_class_W = np.argmax(y_pred_W,axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_class_W, y_pred_class_W)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "As you can see, we achieve an incredible accuracy of 96.7% on the prediction task. This was also trained super quickly and with a minimal amount of code. We can see the power of neural networks for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Report\n",
    "\n",
    "Here are the scorse computed based on what we talked about in class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         6\n",
      "          1       1.00      0.91      0.95        11\n",
      "          2       0.93      1.00      0.96        13\n",
      "\n",
      "avg / total       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_class,y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visualization of the confusion matrix for our model. The confusion matrix helps us learn what the model predicted in comparison to the ground truth class. This allows us to identify where the model goes wrong in general. You can learn more about confusion matrices [here](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122aead30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix(y_test_class,y_pred_class)\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Iris Setosa', 'Iris Versicolor', 'Iris Virginica'], title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
