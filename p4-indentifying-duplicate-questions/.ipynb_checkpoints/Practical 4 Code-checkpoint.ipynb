{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aaron Szabo\n",
    "\n",
    "Please replace first_name and last_name with your information. This notebook will be your template that you should follow for this project. Feel free to create any subsections within each section.\n",
    "\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "np.random.seed(1234)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Data\n",
    "\n",
    "There is no code for you to fill out in this section but please make sure you understand what the code is doing so you aren't confused in later parts. If you want to run this code on Colab, you can pass in the links to the CSV rather than the file path.\n",
    "\n",
    "You can find the training and testing data here: https://www.kaggle.com/c/quora-question-pairs/data\n",
    "\n",
    "#### File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = 'train.csv'\n",
    "TEST_CSV = 'test.csv'\n",
    "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "5   5    11    12  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6   6    13    14                                Should I buy tiago?   \n",
       "7   7    15    16                     How can I be a good geologist?   \n",
       "8   8    17    18                    When do you use シ instead of し?   \n",
       "9   9    19    20  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
       "6  What keeps childern active and far from phone ...             0  \n",
       "7          What should I do to be a great geologist?             1  \n",
       "8              When do you use \"&\" instead of \"and\"?             0  \n",
       "9  How do I hack Motorola DCX3400 for free internet?             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "      <td>What you send money to China?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id                                          question1  \\\n",
       "0        0  How does the Surface Pro himself 4 compare wit...   \n",
       "1        1  Should I have a hair transplant at age 24? How...   \n",
       "2        2  What but is the best way to send money from Ch...   \n",
       "3        3                        Which food not emulsifiers?   \n",
       "4        4                   How \"aberystwyth\" start reading?   \n",
       "\n",
       "                                           question2  \n",
       "0  Why did Microsoft choose core m3 and not core ...  \n",
       "1        How much cost does hair transplant require?  \n",
       "2                      What you send money to China?  \n",
       "3                                  What foods fibre?  \n",
       "4                     How their can I start reading?  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Processing\n",
    "\n",
    "In this section we will proccess our data into a format suitable for inputting into a LSTM or more specifically, a Siamese LSTM. The general structure we will follow is:\n",
    "\n",
    "1. Preprocess the text (clean it up)\n",
    "2. Tokenize every word in the text (replace each word with an index number)\n",
    "3. Pad every sequence of indices with zeros to make them all the same length\n",
    "4. Build an embedding matrix that we can use to look up every indices word embedding\n",
    "\n",
    "In this section, you will be responsible for both preprocessing the text in the function `clean_text` and also building the embedding matrix. Fillers for where you should write your code are marked with #YOUR CODE HERE.\n",
    "\n",
    "Rather than training our own word embeddings on the dataset, it is generally better to use a pretrained one since it is much more accurate. In our example, we will be using a Word2Vec model trained on Google News but you are free to use any you would like. Spacy's Glove model is a good choice.\n",
    "\n",
    "You can download the pretrained model here: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "\n",
    "#### Load Pretrained Word2Vec Model\n",
    "\n",
    "We will load the model into a Keyed Vectors file. Using this, we can get the embedding of any word by calling `.word_vec(word)` and we can get all the words in the model's vocabulary through `.vocab`. It is important to note that if the word does not exist in the model's vocabulary, you can NOT get it's embedding and so standard practice is to ignore it or initialize it to a random or zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True, limit=500000)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (TODO)\n",
    "\n",
    "The text was processed in order to clean up the data. \n",
    "* puncuation marks are removed\n",
    "* non-alphabetic 'words' are removed, e.g. \"'s\"\n",
    "* stopwords are removed\n",
    "* verbs are converted to their infinitive form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens2 = [w.translate(table) for w in tokens]\n",
    "    tokens3 = [w for w in tokens2 if w.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens4 = [w for w in tokens3 if not w in stop_words]\n",
    "    porter = PorterStemmer()\n",
    "    tokens5 = [porter.stem(w) for w in tokens4]\n",
    "    detokenizer = MosesDetokenizer()\n",
    "    text = detokenizer.detokenize(tokens5)\n",
    "    \n",
    "    # Return type should be str\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the preprocessing `clean_text` function to every element in the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Training Data\n",
      "Loaded Testing Data\n"
     ]
    }
   ],
   "source": [
    "X_train_1 = [clean_text(x) for x in train_df['question1']]\n",
    "X_train_2 = [clean_text(x) for x in train_df['question2']]\n",
    "labels = train_df['is_duplicate']\n",
    "print('Loaded Training Data')\n",
    "\n",
    "X_test_1 = [clean_text(x) for x in test_df['question1']]\n",
    "X_test_2 = [clean_text(x) for x in test_df['question2']]\n",
    "print('Loaded Testing Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "To avoid manually having to assign indices and filtering out unfrequent words, we can use a Tokenizer to do this for us. It essentially creates a map of every unique word and an assigned index to it. We specify a parameter called `num_words` which says to only care about the top 20000 most frequent words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Building Tokenizer\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(X_train_1 + X_train_2 + X_test_1 + X_test_2)\n",
    "print('Finished Building Tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the tokenizer to the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Tokenizing Training\n",
      "Finished Tokenizing Testing\n"
     ]
    }
   ],
   "source": [
    "train_sequences_1 = tokenizer.texts_to_sequences(X_train_1)\n",
    "train_sequences_2 = tokenizer.texts_to_sequences(X_train_2)\n",
    "print('Finished Tokenizing Training')\n",
    "\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(X_test_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(X_test_2)\n",
    "print('Finished Tokenizing Testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of unique words in tokenizer. Has to be <= 20,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 108604 unique tokens\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad sequences all to the same length of 30 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (404290, 30)\n",
      "Shape of label tensor: (404290,)\n",
      "Finished Padding Training\n",
      "Finished Padding Testing\n"
     ]
    }
   ],
   "source": [
    "train_data_1 = pad_sequences(train_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "train_data_2 = pad_sequences(train_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', train_data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print('Finished Padding Training')\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Finished Padding Testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Matrix (TODO)\n",
    "\n",
    "The embedding matrix is a `n x m` matrix where `n` is the number of words and `m` is the dimension of the embedding. In our case, `m=300` and `n=20000`. We take the min between the number of unique words in our tokenizer and max words in case there are less unique words than the max we specified. \n",
    "\n",
    "Row `i` in the matrix should contain the embedding of the word with index `i` in the tokenizer. An easy way to create this would be to iterate over `word_index.items()` which gives you the word and it's index. Keep in mind that you can't generate an embedding for a word not in your word2vec model vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating embedding matrix\n"
     ]
    }
   ],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in word_index.items():\n",
    "    if index<20000 and word in word2vec.vocab:\n",
    "        embedding_matrix[index,:] = word2vec.word_vec(word)\n",
    "print(\"Finished creating embedding matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Data\n",
    "\n",
    "Here we just format the data into each half for the input (left and right). There is no code to write here but understand what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Creating Training Data\n",
      "Finished Creating Validation Data\n"
     ]
    }
   ],
   "source": [
    "# Random shuffle\n",
    "perm = np.random.permutation(len(train_data_1))\n",
    "idx_train = perm[:int(len(train_data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(train_data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack((train_data_1[idx_train], train_data_2[idx_train]))\n",
    "data_2_train = np.vstack((train_data_2[idx_train], train_data_1[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "print('Finished Creating Training Data')\n",
    "\n",
    "data_1_val = np.vstack((train_data_1[idx_val], train_data_2[idx_val]))\n",
    "data_2_val = np.vstack((train_data_2[idx_val], train_data_1[idx_val]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "print('Finished Creating Validation Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Building the Model\n",
    "\n",
    "In this section you will write code to build the actual Siamese network. It should take in two arguments (question1 and question2) and then output a single number representing the probability that the two questions are duplicates.\n",
    "\n",
    "The model should take in each input sentence, replace it with it's embeddings, then run the new embedding vector through a LSTM layer. The output of each LSTM layer should be concatenated together and then a standard Dense model can be used.\n",
    "\n",
    "Make sure to note that you should only use one LSTM layer that is shared by both the left and the right half. \n",
    "\n",
    "Make sure to title your output layers as `predictions`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building model\n"
     ]
    }
   ],
   "source": [
    "left_input = Input(shape=(30,), dtype='int32')\n",
    "right_input = Input(shape=(30,), dtype='int32')\n",
    "embedding_layer = Embedding(20000, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_NB_WORDS, trainable=False)\n",
    "left_embedded = embedding_layer(left_input)\n",
    "right_embedded = embedding_layer(right_input)\n",
    "left_model = Dropout(0.2)(left_embedded)\n",
    "right_model = Dropout(0.2)(right_embedded)\n",
    "model = LSTM(100)\n",
    "left_output = model(left_model)\n",
    "right_output = model(right_model)\n",
    "left_output = Dropout(0.2)(left_output)\n",
    "right_output = Dropout(0.2)(right_output)\n",
    "predictions = concatenate([left_output, right_output])\n",
    "predictions = Dense(1,activation=\"sigmoid\")(predictions)\n",
    "print(\"Finished building model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[left_input, right_input], outputs=predictions)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Training the Model\n",
    "\n",
    "In this section we will simply train the model. We use the Early Stopping argument to end training if the loss or accuracy don't improve within 3 epochs.\n",
    "\n",
    "Since the training time is incredibly long (30 minutes or so on a CPU), only train it for one epoch if you don't have time. For better results, train it to around 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 687292 samples, validate on 121288 samples\n",
      "Epoch 1/10\n",
      "687292/687292 [==============================] - 520s 757us/step - loss: 0.5459 - acc: 0.7249 - val_loss: 0.5360 - val_acc: 0.7327\n",
      "Epoch 2/10\n",
      "687292/687292 [==============================] - 516s 751us/step - loss: 0.5275 - acc: 0.7373 - val_loss: 0.5232 - val_acc: 0.7427\n",
      "Epoch 3/10\n",
      "687292/687292 [==============================] - 516s 751us/step - loss: 0.5138 - acc: 0.7463 - val_loss: 0.5168 - val_acc: 0.7474\n",
      "Epoch 4/10\n",
      "687292/687292 [==============================] - 516s 750us/step - loss: 0.5019 - acc: 0.7538 - val_loss: 0.5165 - val_acc: 0.7490\n",
      "Epoch 5/10\n",
      "687292/687292 [==============================] - 515s 750us/step - loss: 0.4910 - acc: 0.7599 - val_loss: 0.5165 - val_acc: 0.7467\n",
      "Epoch 6/10\n",
      "687292/687292 [==============================] - 516s 751us/step - loss: 0.4811 - acc: 0.7657 - val_loss: 0.5036 - val_acc: 0.7584\n",
      "Epoch 7/10\n",
      "687292/687292 [==============================] - 516s 751us/step - loss: 0.4711 - acc: 0.7710 - val_loss: 0.5050 - val_acc: 0.7596\n",
      "Epoch 8/10\n",
      "687292/687292 [==============================] - 516s 751us/step - loss: 0.4626 - acc: 0.7756 - val_loss: 0.5045 - val_acc: 0.7620\n",
      "Epoch 9/10\n",
      "687292/687292 [==============================] - 526s 765us/step - loss: 0.4547 - acc: 0.7796 - val_loss: 0.5059 - val_acc: 0.7645\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val], labels_val), \\\n",
    "        epochs=10, batch_size=2048, shuffle=True, \\\n",
    "        callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.7248956775574119,\n",
       "  0.737347444789578,\n",
       "  0.7463378011276666,\n",
       "  0.7537713228529156,\n",
       "  0.7598677127227382,\n",
       "  0.7657153000235526,\n",
       "  0.7710099928914236,\n",
       "  0.7756310272638484,\n",
       "  0.7795856782432378],\n",
       " 'loss': [0.5459290541220257,\n",
       "  0.5274997776135069,\n",
       "  0.5138215278342549,\n",
       "  0.5019437526635017,\n",
       "  0.4909860180424219,\n",
       "  0.48105141785025435,\n",
       "  0.47110257770265135,\n",
       "  0.46256982635025423,\n",
       "  0.4546618327479964],\n",
       " 'val_acc': [0.7326858386020926,\n",
       "  0.7427033176955852,\n",
       "  0.7473698965470431,\n",
       "  0.7490188641320448,\n",
       "  0.7467103093856636,\n",
       "  0.758376756127061,\n",
       "  0.7595887474834144,\n",
       "  0.7619550161048437,\n",
       "  0.7645438954309803],\n",
       " 'val_loss': [0.5359862937193481,\n",
       "  0.5231903300193501,\n",
       "  0.5168004283984602,\n",
       "  0.5164676649370276,\n",
       "  0.5164755796601453,\n",
       "  0.5035652156952904,\n",
       "  0.5049841041953683,\n",
       "  0.5044520871340147,\n",
       "  0.5059474569029532]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_hist2 = hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.6950262770065657],\n",
       " 'loss': [0.5812028301110251],\n",
       " 'val_acc': [0.715289228970202],\n",
       " 'val_loss': [0.5610955431541379]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
